{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Non-parametric Classifiers\n",
    "Machine Learning 2023/2024 <br>\n",
    "CSE Machine Learning Teaching Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on K-NN density estimation.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of non-parametric classifiers.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a fellow student. If you want to skip right to the questions and exercises, find the $\\rightarrow$ symbol. For questions and feedback please consult the TAs during the lab session. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours\n",
    "\n",
    "Last week, you got acquainted with parametric classifiers. You described a distribution using only a few parameters (mean and standard deviation) and tried to find the values for those parameters that best fit the data. This week, you will work on non-parametric classifiers. As the name implies, these classifiers do not use parameters to describe their data. Instead, they directly use training data in the classification process, or set up rules to classify new samples.\n",
    "\n",
    "A popular example of non-parametric classifiers is the K-Nearest Neighbours (K-NN) classifier. In this exercise, you will find out how it works by implementing it yourself and you'll get to know in what circumstances to use it. In this assignment, we will walk you through the following steps in the K-NN algorithm:\n",
    "\n",
    "1. Load data: Open the dataset from CSV and split into test/train datasets.\n",
    "2. Similarity: Calculate the distance between two data instances.\n",
    "3. Nearest Neighbours: Locate k most similar data instances.\n",
    "4. Majority vote: Get the neighbours to vote on the class of the test points.\n",
    "5. Accuracy: Summarize the accuracy of predictions.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "In this notebook we will work with the Iris dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets # To load the dataset\n",
    "from sklearn.model_selection import train_test_split # To split in train and test set\n",
    "\n",
    "seed = 20\n",
    "# Load the data and create the training and test sets\n",
    "iris = datasets.load_iris()\n",
    "# X is the feature vectors of the data points, and Y is the target (ground truth) class for those data points \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Print and plot the data to understand what you will be classifying. You can plot points using Matplotlib's [scatter](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.scatter.html) function. We have already imported `pyplot` as `plt`. This means you can call the `scatter` function with `plt.scatter(x, y, ...)`.\n",
    "\n",
    "__Hint:__ Each sample has four features (corresponding to the length and the width of the sepals and petals), you can just plot two of them to get a basic idea of the data. The feature names can be accessed in the list *iris_feature_names*.<br>\n",
    "__Hint:__ Use the `c=Y_train` parameter to colour each point with its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ Would it work to classify this dataset with a parametric classifier? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity\n",
    "\n",
    "Next, we will create a function to compute distance between two points $ \\mathbf{p}$ and $\\mathbf{q} $. We will employ the often used Euclidean distance to find the nearest neighbours of a point.\n",
    "\n",
    "$\\ex{2.1}$ Complete the `euclidean` function. This function should compute the Euclidean distance between two points (i.e. feature vectors). \n",
    "\n",
    "**Note:** As we are working with feature vectors, the \"$\\cdot$\" depicts a dotproduct:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{(\\mathbf{p} - \\mathbf{q})\\cdot(\\mathbf{p} - \\mathbf{q})}\n",
    "$$\n",
    "\n",
    "__Hint:__ You might know a more specific formulation of this as $|\\mathbf{p}| = \\sqrt{p_1^2 + p_2^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def euclidean(p, q):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between point p and q.\n",
    "    :param p: point p as a numpy array.\n",
    "    :param q: point q as a numpy array.\n",
    "    :return: distance as float.\n",
    "    \"\"\"\n",
    "    \n",
    "    dist = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return dist\n",
    "\n",
    "# Check whether your algorithm is correct\n",
    "a = np.array([2, 4, 8])\n",
    "b = np.array([3, 5, 9])\n",
    "\n",
    "print('The output of your algorithm:', euclidean(a, b))\n",
    "assert np.isclose(euclidean(a, b), distance.euclidean(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.1}$ Could you name a few other distance functions? What would be the effect of choosing another distance function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nearest Neighbours\n",
    "\n",
    "Now that we can define a distance between points, we will try to find the $k$ (e.g. 5) nearest neighbours in the training set for a test instance. These nearest neighbours give us information about the class that a test instance is likely to belong to.\n",
    "\n",
    "$\\q{3.1}$ Given $n$ training samples and $m$ test instances, express the number of steps (complexity) this would take in big-O notation: $O(...)$.\n",
    "\n",
    "$\\ex{3.1}$ Complete the `get_neighbours` function.\n",
    "\n",
    "__Challenge__ If you are limited by storage to $O(k)$, what datastructure would you use to store the $k$ nearest neighbours? Can you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(training_set, test_instance, k):\n",
    "    \"\"\"\n",
    "    Calculate distances from test_instance to all training points.\n",
    "    :param training_set: [n x d] numpy array of training samples (n: number of samples, d: number of dimensions).\n",
    "    :param test_instance: [d x 1] numpy array of test instance features.\n",
    "    :param k: number of neighbours to return.\n",
    "    :return: list of length k with neighbour indices, with increasing distance of the neighbours\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbours = []\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return neighbours\n",
    "\n",
    "neighbours = get_neighbours(X_train, X_test[0], 5)\n",
    "\n",
    "# Check whether your algorithm is correct\n",
    "print('The indices returned by your algorithm are:', neighbours)\n",
    "assert neighbours == [63, 41, 76, 51, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our implementation is correct by plotting the points in 2D.\n",
    "\n",
    "$\\ex{3.2}$ Use the provided plot code to show the nearest neighbours for a couple of different values for $k$ and a number of test samples. Is your function working?\n",
    "\n",
    "__Hint:__ Remember that the dataset contains four features. However, for the purpose of grasping the concept of nearest neighbours, only two features are used to create the plot. What does this mean for the nearest neighbors of the test instances shown in the plot?\n",
    "\n",
    "__Hint:__ The larger datapoints are the k-NNs of the test point. Note that these do not have to be of the same color (even though in this case they are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbours(X_train, Y_train, test_instance, k):\n",
    "    \"\"\"\n",
    "    Plots all points in the dataset and shows the neighbours of a given test instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbours = get_neighbours(X_train, test_instance, k)\n",
    "    # Initialization of the sizes of the points to be plotted, size 10 \n",
    "    neigh_sizes = np.ones((len(Y_train), 1)) * 10\n",
    "    neigh_sizes[neighbours] = 50\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, s=neigh_sizes)\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.colorbar(ticks = [0, 1, 2], format = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)]));\n",
    "    plt.scatter(test_instance[0], test_instance[1], c='r', s=50, marker='x')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    test_instance = X_test[i, [0, 1]]\n",
    "    k = 5\n",
    "    plt.title('Test instance %s and its nearest neighbors' % (i+1))\n",
    "    plot_neighbours(X_train[:, [0,1]], Y_train, test_instance, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Majority vote\n",
    "\n",
    "We have the $k$ nearest neighbours of the test set. Now we will choose a class label by majority vote.\n",
    "\n",
    "$\\ex{4.1}$ Implement the `get_majority_vote` function.\n",
    "\n",
    "_Hint:_ In the case of a split vote, pick the one that is closest.\n",
    "\n",
    "_Hint:_ We imported a `Counter` which can help you tally up the votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # To count unique occurrences of items in array, for majority voting\n",
    "\n",
    "def get_majority_vote(neighbour_indices, training_labels):\n",
    "    \"\"\"\n",
    "    Given an array of nearest neighbours indices for a given test case, \n",
    "    tally up their classes to vote on the correct class for the test instance.\n",
    "    :param neighbours: list of nearest neighbour indices.\n",
    "    :param training_labels: the list of labels for each training instance.\n",
    "    :return: the label of most common class.\n",
    "    \"\"\"\n",
    "    \n",
    "    most_common = -1\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return most_common\n",
    "\n",
    "predicted_label = get_majority_vote(neighbours, Y_train)\n",
    "print('Your predicted label:', predicted_label)\n",
    "\n",
    "assert predicted_label == 0\n",
    "assert get_majority_vote([0,1,2,3,4], [0,2,2,1,3]) == 2\n",
    "assert get_majority_vote([0,1,2,3,4], [3,1,1,3,0]) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Accuracy\n",
    "\n",
    "Now we will put all the code you wrote above together into a classifier and try to summarise the accuracy of it with the test set.\n",
    "\n",
    "$\\ex{5.1}$ Complete the `predict` function below and compute the accuracy on `X_test, Y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict(X_train, X_test, Y_train, k=5):\n",
    "    \"\"\"\n",
    "    Predicts all labels for the test set, using k-nn on the training set.\n",
    "    :param X_train: the training set features.\n",
    "    :param X_test: the test set features.\n",
    "    :param y_train: the training set labels.\n",
    "    :return: list of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    # For each instance in the test set, get nearest neighbours and majority vote on predicted class\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return predictions\n",
    "\n",
    "k = 5\n",
    "predictions = predict(X_train, X_test, Y_train, k)\n",
    "\n",
    "# Summarise performance of the classification using scikit-learn\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print('The overall accuracy of the model using scikit-learn is:', accuracy)\n",
    "\n",
    "assert predictions == [0, 1, 1, 2, 1, 1, 2, 0, 2, 0, 2, 1, 2, 0, 0, 2, 0, 1, 2, 1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 2, 1, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 2, 1, 1, 1, 0, 0, 1]\n",
    "assert np.isclose(accuracy, 0.9666666666666667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ Complete the `accuracy_score_self` and use this to compute your own accuracy, which should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_self(Y_test, predictions):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a test set as the fraction of items that was classified correctly.\n",
    "    :param y_test: the list of true labels for the test set.\n",
    "    :param y_pred: the list of predicted labels for the test set.\n",
    "    :return: accuracy as a floating point.\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy = 0\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return accuracy\n",
    "\n",
    "# Summarise performance of the classification\n",
    "accuracy_self = accuracy_score_self(Y_test, predictions)\n",
    "print('The overall accuracy of the model using your implementation of accuracy:', accuracy_self)\n",
    "assert np.isclose(accuracy, accuracy_self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.3}$ Complete the `plot_errors` function to get a better understanding of why some points are misclassified. You can use the `plot_neighbours` function you made earlier to make a plot per misclassified point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(X_train, X_test, Y_train, Y_test, predictions, k):\n",
    "    \"\"\"\n",
    "    Plots the test points that were misclassified and their nearest neighbours using plot_neighbours.\n",
    "    \"\"\"\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return\n",
    "\n",
    "plot_errors(X_train, X_test, Y_train, Y_test, predictions, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.4}$ Test out several values of $k$ to find the best performing $k$. Automate this process: for $k = 1...20$ compute the average accuracy over 10 repetitions (to average over randomness in train/test splits, i.e. [cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)) and plot the accuracy for each $k$.\n",
    "\n",
    "__Hint:__ You can store all the results in a `[max_neighbours x n_repetitions]` NumPy array and use `np.mean` with `axis=1` to compute the mean accross the number of repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repetitions = 10\n",
    "max_neighbours = 20\n",
    "accuracies = np.zeros((max_neighbours, n_repetitions))\n",
    "mean_accuracies = np.zeros(max_neighbours)\n",
    "seeds = [x for x in range(n_repetitions)]\n",
    "\n",
    "for i in range(n_repetitions):\n",
    "    # Generate a new split of train and testset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=seeds[i])\n",
    "    for k in range(1, max_neighbours + 1):\n",
    "    # START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "plt.plot(range(1, 21), mean_accuracies)\n",
    "plt.title('The averaged accuracies over the different values of k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.2}$ What $k$ would you pick, based on your results? Does it matter a lot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are plotting things to learn about our classifier, let's take a brief look at learning curves as well. For a learning curve, we plot the number of samples (x-axis) in the train set against the accuracy (y-axis).\n",
    "\n",
    "$\\q{5.3}$ What would you expect the learning curve to look like for the k-NN classifier?\n",
    "\n",
    "Let's go ahead and create a learning curve.\n",
    "\n",
    "$\\ex{5.5}$ Read through the code to understand what is happening and execute to plot the learning curve. Try this for several values for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=seed)\n",
    "\n",
    "total_samples = X_train.shape[0]\n",
    "# Set up array to store accuracies\n",
    "accuracies = np.zeros(total_samples + 1)\n",
    "\n",
    "# We want to learn with at least k samples and up to the size of the train set\n",
    "for i in range(k, total_samples):\n",
    "    predictions = predict(X_train[:i], X_test, Y_train[:i], k)\n",
    "    accuracies[i + 1] = accuracy_score(Y_test, predictions)\n",
    "    \n",
    "# Plot learning curve\n",
    "plt.plot(range(total_samples + 1), accuracies)\n",
    "plt.title('The learning curve for the k-NN classifier')\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.4}$ Did the learning curve resemble the expected curve? If not, why?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e6356280a9f989dced758274f9f263cb0f62e1270609ec0527f1ba4aaa42f4e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
